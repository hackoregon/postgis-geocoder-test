---
title: "Hack Oregon Transportation Data Cleaning / Geocoding"
output:
  github_document: default
  html_notebook: default
---

## Source data
The original source data came from the City of Portland. We received two PDFs for "grind and pave" projects, which appear to contain identical tables. We converted them manually to the CSV files here using [Tabula](http://tabula.technology/). If we receive more PDFs, Tabula has a command-line option that can be scripted from most higher-level languages, including both Python and R.

The remainder of the files were received as comma-separated-value (CSV) files. The process of uploading them to Google Drive and downloading them again converted them to Microsoft Excel(.xlsx) format. We converted them back to the CSV files here manually using LibreOffice. Again, if we receive more such files we can automate the processing.

## Tidying the data
Once converted to CSV, inspection shows that the data are in multiple formats. The geocoding process requires a tidy set of inputs, so we define a tidy format and populate it with data from the files.

### Inputs
First, we import the CSV files to individual data frames.

```{r}
library(readr)
Pavement_Moratorium <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Pavement Moratorium.csv",
  col_types = cols(
    `Moratorium End Date` = col_date(format = "%m/%d/%Y"),
    OBJECTID = col_character(), 
    `Treatment Date` = col_date(format = "%m/%d/%Y")))
print(Pavement_Moratorium)

Planned_Fog_Seal <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Planned Fog Seal.csv",
  col_types = cols(
    OBJECTID = col_character()))
print(Planned_Fog_Seal)

Planned_Paving <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Planned Paving.csv",
  col_types = cols(OBJECTID = col_character()))

tabula_G_P_Schedule_as_of_1_5_2017 <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/tabula-G_P Schedule as of 1-5-2017.csv",
  col_types = cols(Start = col_character()))
print(tabula_G_P_Schedule_as_of_1_5_2017)

```
### Outputs
In theory, all we have to do is extract the intersections from these files into a single table and pass it on to the PostGIS geocoders `geocode_intersection` operation. However, we want to tag the input table rows with identifiers so the people reading the output will know where the inputs came from.

In addition, most of the Hack Oregon processing works with geometric / geographic objects in GeoJSON format, so we want to create GeoJSON representations of the geocoded intersections for downstream processing.

So our input table to the PostGIS geocoder will look like this:

```{r}
library(tibble)
print(tribble(
  ~source_file_name, ~source_row_number, ~street, ~cross_street, ~from_or_to,
  "file", 1, "Main St", "State St", "from"))

```

and the output will have three more columns on the right: lon (longitude), lat (latitude), and geojson (text serialized GeoJSON object for the intersection).

## Parsers

### Grind and Pave
This is the most complicated parser, since the "Street", "From" and "To" fields are all given in a single column. The first order of business is to parse the `Task Name` column.

```{r}
library(dplyr)
grind_and_pave <- tabula_G_P_Schedule_as_of_1_5_2017 %>%
  mutate(source_file_name = "G_P Schedule as of 1-5-2017.pdf") %>%
  rownames_to_column(var = "source_row_number") %>%
  select(source_file_name, source_row_number, `Task Name`) %>%
  filter(!is.na(`Task Name`), `Task Name` != "FY 17/18") %>%
  mutate(
    street = sub(":.*$", "", `Task Name`),
    from = gsub("\r", " ", `Task Name`) %>%
      sub("^.*:", "", .) %>%
      sub(" to .*$", "", .),
    to = gsub("\r", " ", `Task Name`) %>%
      sub("^.* to ", "", .)) %>%
  select(-`Task Name`)
grind_and_pave$source_row_number <- as.integer(grind_and_pave$source_row_number)
```
### Pavement moratorium
This one's easier - the only non-tidy feature is that some of the `Street` entries have a " Base-Repair" or similar tacked onto the end.

```{r}
pavement_moratorium <- Pavement_Moratorium %>%
  mutate(
    source_file_name = "Pavement Moratorium.csv",
    street = sub("Base.*$", "", Street) %>% sub("-$", "", .)) %>%
  rownames_to_column(var = "source_row_number") %>%
  select(
    source_file_name, 
    source_row_number, 
    street, 
    from = `From Street`,
    to = `To Street`)
pavement_moratorium$source_row_number <- as.integer(pavement_moratorium$source_row_number)
```

### Planned fog seal
```{r}
planned_fog_seal <- Planned_Fog_Seal %>%
  mutate(source_file_name = "Planned Fog Seal.csv") %>%
  rownames_to_column(var = "source_row_number") %>%
  select(
    source_file_name, 
    source_row_number, 
    street = Street, 
    from = `From Street`,
    to = `To Street`)
planned_fog_seal$source_row_number <- as.integer(planned_fog_seal$source_row_number)
```

### Planned paving
```{r}
planned_paving <- Planned_Paving %>%
  mutate(source_file_name = "Planned Paving.csv") %>%
  rownames_to_column(var = "source_row_number") %>%
  select(
    source_file_name, 
    source_row_number, 
    street = Street, 
    from = `From Street`,
    to = `To Street`)
planned_paving$source_row_number <- as.integer(planned_paving$source_row_number)
```

## Collection for geocoding
### Bind rows to one data frame
```{r}
geocoder_input <- bind_rows(grind_and_pave, pavement_moratorium, planned_fog_seal, planned_paving)
```

### Addresses, intersections and lines
If you look at `geocoder_input`, you'll notice some entries have no `from` or `to` value, and some have a `from` but no `to`. They'll have different geocoding requirements. So we divide them into three data frames via `filter`.
```{r}
no_cross_streets <- geocoder_input %>% filter(is.na(from) & is.na(to))
write_csv(no_cross_streets, path = "no_cross_streets.csv")
from_only <- geocoder_input %>% filter(!is.na(from) & is.na(to))
write_csv(from_only, path = "from_only.csv")
both <- geocoder_input %>% filter(!is.na(from) & !is.na(to))
write_csv(both, path = "both.csv")
```
