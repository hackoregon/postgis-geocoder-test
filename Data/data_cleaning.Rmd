---
title: "Hack Oregon Transportation Data Cleaning / Geocoding"
output: html_notebook
---

## Source data
The original source data came from the City of Portland. We received two PDFs for "grind and pave" projects, which appear to contain identical tables. We converted them manually to the CSV files here using [Tabula](http://tabula.technology/). If we receive more PDFs, Tabula has a command-line option that can be scripted from most higher-level languages, including both Python and R.

The remainder of the files were received as comma-separated-value (CSV) files. The process of uploading them to Google Drive and downloading them again converted them to Microsoft Excel(.xlsx) format. We converted them back to the CSV files here manually using LibreOffice. Again, if we receive more such files we can automate the processing.

## Tidying the data
Once converted to CSV, inspection shows that the data are in multiple formats. The geocoding process requires a tidy set of inputs, so we define a tidy format and populate it with data from the files.

### Inputs
First, we import the CSV files to individual data frames.

```{r}
library(readr)
Pavement_Moratorium <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Pavement Moratorium.csv",
  col_types = cols(
    `Moratorium End Date` = col_date(format = "%m/%d/%Y"),
    OBJECTID = col_character(), 
    `Treatment Date` = col_date(format = "%m/%d/%Y")))
print(Pavement_Moratorium)

Planned_Fog_Seal <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Planned Fog Seal.csv",
  col_types = cols(
    OBJECTID = col_character()))
print(Planned_Fog_Seal)

Planned_Paving <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/Planned Paving.csv",
  col_types = cols(OBJECTID = col_character()))

tabula_G_P_Schedule_as_of_1_5_2017 <- read_csv(
  "/home/Projects/postgis-geocoder-test/Data/tabula-G_P Schedule as of 1-5-2017.csv",
  col_types = cols(Start = col_character()))
print(tabula_G_P_Schedule_as_of_1_5_2017)

```
### Outputs
In theory, all we have to do is extract the intersections from these files into a single table and pass it on to the PostGIS geocoders `geocode_intersection` operation. However, we want to tag the input table rows with identifiers so the people reading the output will know where the inputs came from.

In addition, most of the Hack Oregon processing works with geometric / geographic objects in GeoJSON format, so we want to create GeoJSON representations of the geocoded intersections for downstream processing.

So our input table to the PostGIS geocoder will look like this:

```{r}
library(tibble)
print(tribble(
  ~source_file_name, ~source_row_number, ~street, ~cross_street, ~from_or_to,
  "file", 1, "Main St", "State St", "from"))

```

and the output will have three more columns on the right: lon (longitude), lat (latitude), and geojson (text serialized GeoJSON object for the intersection).

